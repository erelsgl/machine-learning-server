{
  "name": "limdu",
  "description": "A machine learning framework for Node.js. Supports multi-level classification and online learning.",
  "version": "0.3.0",
  "author": {
    "name": "Erel Segal-haLevi",
    "email": "erelsgl@gmail.com"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/erelsgl/limdu.git"
  },
  "dependencies": {
    "underscore": "*",
    "sprintf": "*",
    "svm": "*",
    "temp": "*",
    "execSync": "*",
    "brain": "*",
    "graph-paths": "*",
    "languagemodel": "*"
  },
  "devDependencies": {
    "mocha": ">=1.0.0",
    "should": ">=0.6.0"
  },
  "scripts": {
    "test": "mocha"
  },
  "keywords": [
    "classifier",
    "classification",
    "categorization",
    "text classification",
    "natural lanaguage understanding",
    "machine learning",
    "multi label",
    "multilabel",
    "multi class",
    "multiclass",
    "online learning",
    "naive bayes",
    "winnow",
    "perceptron",
    "binary relevance",
    "one vs all"
  ],
  "readme": "# Limdu\n\nLimdu is a machine-learning framework for Node.js, which supports online learning and multi-label classification.\n\n## Installation\n\n\tnpm install limdu\n\n## Demos\n\nWorking demos can be found at [limdu-demo](https://github.com/erelsgl/limdu-demo).\n\n**Table of Contents**  *generated with [DocToc](http://doctoc.herokuapp.com/)*\n\n- [Limdu](#limdu)\n\t- [Installation](#installation)\n\t- [Demos](#demos)\n\t- [Binary Classification](#binary-classification)\n\t\t- [Batch Learning - learn from an array of input-output pairs:](#batch-learning---learn-from-an-array-of-input-output-pairs)\n\t\t- [Online Learning; Explanations](#online-learning-explanations)\n\t\t- [Binding](#binding)\n\t\t- [Other Binary Classifiers](#other-binary-classifiers)\n\t- [Multi-Label Classification](#multi-label-classification)\n\t\t- [Other Multi-label classifiers](#other-multi-label-classifiers)\n\t- [Feature engineering](#feature-engineering)\n\t\t- [Feature extraction - converting an input sample into feature-value pairs:](#feature-extraction---converting-an-input-sample-into-feature-value-pairs)\n\t\t- [Input Normalization](#input-normalization)\n\t\t- [Feature lookup table - convert custom features to integer features](#feature-lookup-table---convert-custom-features-to-integer-features)\n\t- [Serialization](#serialization)\n\t- [Cross-validation](#cross-validation)\n\t- [Back-classification (aka Generation)](#back-classification-aka-generation)\n\t- [SVM wrappers](#svm-wrappers)\n\t- [Undocumented featuers](#undocumented-featuers)\n\t- [Contributions](#contributions)\n\t- [License](#license)\n\n## Binary Classification\n\n### Batch Learning - learn from an array of input-output pairs:\n\n\tvar limdu = require('limdu');\n\t\n\tvar colorClassifier = new limdu.classifiers.NeuralNetwork();\n\t\n\tcolorClassifier.trainBatch([\n\t\t{input: { r: 0.03, g: 0.7, b: 0.5 }, output: 0},  // black\n\t\t{input: { r: 0.16, g: 0.09, b: 0.2 }, output: 1}, // white\n\t\t{input: { r: 0.5, g: 0.5, b: 1.0 }, output: 1}   // white\n\t\t]);\n\t\n\tconsole.log(colorClassifier.classify({ r: 1, g: 0.4, b: 0 }));  // 0.99 - almost white\n\nCredit: this example uses [brain.js, by Heather Arthur](https://github.com/harthur/brain).\n\n\n### Online Learning; Explanations\n\n\tvar limdu = require('limdu');\n\t\n\tvar birdClassifier = new limdu.classifiers.Winnow({\n\t\tdefault_positive_weight: 1,\n\t\tdefault_negative_weight: 1,\n\t\tthreshold: 0\n\t});\n\t\n\tbirdClassifier.trainOnline({'wings': 1, 'flight': 1, 'beak': 1, 'eagle': 1}, 1);  // eagle is a bird (1)\n\tbirdClassifier.trainOnline({'wings': 0, 'flight': 0, 'beak': 0, 'dog': 1}, 0);    // dog is not a bird (0)\n\tconsole.dir(birdClassifier.classify({'wings': 1, 'flight': 0, 'beak': 0.5, 'penguin':1})); // initially, penguin is mistakenly classified as 0 - \"not a bird\"\n\tconsole.dir(birdClassifier.classify({'wings': 1, 'flight': 0, 'beak': 0.5, 'penguin':1}, /*explanation level=*/4)); // why? because it does not fly.\n\n\tbirdClassifier.trainOnline({'wings': 1, 'flight': 0, 'beak': 1, 'penguin':1}, 1);  // learn that penguin is a bird, although it doesn't fly \n\tbirdClassifier.trainOnline({'wings': 0, 'flight': 1, 'beak': 0, 'bat': 1}, 0);     // learn that bat is not a bird, although it does fly\n\tconsole.dir(birdClassifier.classify({'wings': 1, 'flight': 0, 'beak': 1, 'chicken': 1})); // now, chicken is correctly classified as a bird, although it does not fly.  \n\tconsole.dir(birdClassifier.classify({'wings': 1, 'flight': 0, 'beak': 1, 'chicken': 1}, /*explanation level=*/4)); // why?  because it has wings and beak.\n\nCredit: this example uses Modified Balanced Margin Winnow ([Carvalho and Cohen, 2006](http://www.citeulike.org/user/erelsegal-halevi/article/2243777)):\n\n### Binding\n\nUsing Javascript's binding capabilities, it is possible to create custom classes, which are made of existing classes and pre-specified parameters:\n\n\tvar MyWinnow = limdu.classifiers.Winnow.bind(0, {\n\t\tdefault_positive_weight: 1,\n\t\tdefault_negative_weight: 1,\n\t\tthreshold: 0\n\t});\n\t\n\tvar birdClassifier = new MyWinnow();\n\t...\n\t// continue as above\n\n\n### Other Binary Classifiers\n\nIn addition to Winnow and NeuralNetwork, version 0.2 includes the following binary classifiers:\n\n* Bayesian - uses [classifier.js, by Heather Arthur](https://github.com/harthur/classifier). \n* Perceptron - Loosely based on [perceptron.js, by  by John Chesley](https://github.com/chesles/perceptron)\n* SVM - uses [svm.js, by Andrej Karpathy](https://github.com/karpathy/svmjs). \n* Linear SVM - wrappers around SVM-Perf and Lib-Linear (see below).\n\nThis library is still under construction, and not all features work for all classifiers. For a full list of the features that do work, see the \"test\" folder. \n\n\n## Multi-Label Classification\n\nIn binary classification, the output is 0 or 1;\n\nIn multi-label classification, the output is a set of zero or more labels.\n\n\tvar MyWinnow = limdu.classifiers.Winnow.bind(0, {retrain_count: 10});\n\n\tvar intentClassifier = new limdu.classifiers.multilabel.BinaryRelevance({\n\t\tbinaryClassifierType: MyWinnow\n\t});\n\t\n\tintentClassifier.trainBatch([\n\t\t{input: {I:1,want:1,an:1,apple:1}, output: \"APPLE\"},\n\t\t{input: {I:1,want:1,a:1,banana:1}, output: \"BANANA\"},\n\t\t{input: {I:1,want:1,chips:1}, output: \"CHIPS\"}\n\t\t]);\n\n\tconsole.dir(intentClassifier.classify({I:1,want:1,an:1,apple:1,and:1,a:1,banana:1}));  // ['APPLE','BANANA']\n\n### Other Multi-label classifiers\n\nIn addition to BinaryRelevance, version 0.2 includes the following multi-label classifier types (see the multilabel folder):\n\n* Cross-Lingual Language Model Classifier (based on [Anton Leusky and David Traum, 2008](http://www.citeulike.org/user/erelsegal-halevi/article/12540655))\n* HOMER - Hierarchy Of Multi-label classifiERs (based on [Tsoumakas et al., 2007](http://www.citeulike.org/user/erelsegal-halevi/article/3170786))\n* Meta-Labeler (based on [Lei Tang, Suju Rajan, Vijay K. Narayanan, 2009](http://www.citeulike.org/user/erelsegal-halevi/article/4860265)) \n* Joint identification and segmentation (based on [Fabrizio Morbini, Kenji Sagae, 2011](http://www.citeulike.org/user/erelsegal-halevi/article/10259046))\n* Passive-Aggressive (based on [Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, Yoram Singer, 2006](http://www.citeulike.org/user/erelsegal-halevi/article/5960770))\n\nThis library is still under construction, and not all features work for all classifiers. For a full list of the features that do work, see the \"test\" folder. \n\n## Feature engineering\n\n### Feature extraction - converting an input sample into feature-value pairs:\n\n\t// First, define our base classifier type (a multi-label classifier based on winnow):\n\tvar TextClassifier = limdu.classifiers.multilabel.BinaryRelevance.bind(0, {\n\t\tbinaryClassifierType: limdu.classifiers.Winnow.bind(0, {retrain_count: 10})\n\t});\n\t\n\t// Now define our feature extractor - a function that takes a sample and adds features to a given features set:\n\tvar WordExtractor = function(input, features) {\n\t\tinput.split(\" \").forEach(function(word) {\n\t\t\tfeatures[word]=1;\n\t\t});\n\t};\n\t\n\t// Initialize a classifier with the base classifier type and the feature extractor:\n\tvar intentClassifier = new limdu.classifiers.EnhancedClassifier({\n\t\tclassifierType: TextClassifier,\n\t\tfeatureExtractor: WordExtractor\n\t});\n\t\n\t// Train and test:\n\tintentClassifier.trainBatch([\n\t\t{input: \"I want an apple\", output: \"apl\"},\n\t\t{input: \"I want a banana\", output: \"bnn\"},\n\t\t{input: \"I want chips\", output:    \"cps\"},\n\t\t]);\n\t\n\tconsole.dir(intentClassifier.classify(\"I want an apple and a banana\"));  // ['apl','bnn']\n\tconsole.dir(intentClassifier.classify(\"I WANT AN APPLE AND A BANANA\"));  // []\n\t\nAs you can see from the last example, by default feature extraction is case-insensitive. \nWe will take care of this in the next example.\n\nInstead of defining your own feature extractor, you can use those already bundled with limdu:\n\n\tlimdu.features.NGramsOfWords\n\tlimdu.features.NGramsOfLetters\n\tlimdu.features.HypernymExtractor\n\nYou can also make 'featureExtractor' an array of several feature extractors, that will be executed in the order you include them.\n\n### Input Normalization\n\n\t//Initialize a classifier with a feature extractor and a case normalizer:\n\tintentClassifier = new limdu.classifiers.EnhancedClassifier({\n\t\tclassifierType: TextClassifier,  // same as in previous example\n\t\tnormalizer: limdu.features.LowerCaseNormalizer,\n\t\tfeatureExtractor: WordExtractor  // same as in previous example\n\t});\n\n\t//Train and test:\n\tintentClassifier.trainBatch([\n\t\t{input: \"I want an apple\", output: \"apl\"},\n\t\t{input: \"I want a banana\", output: \"bnn\"},\n\t\t{input: \"I want chips\", output: \"cps\"},\n\t\t]);\n\t\n\tconsole.dir(intentClassifier.classify(\"I want an apple and a banana\"));  // ['apl','bnn']\n\tconsole.dir(intentClassifier.classify(\"I WANT AN APPLE AND A BANANA\"));  // ['apl','bnn'] \n\nOf course you can use any other function as an input normalizer. You can also make 'normalizer' an array of several normalizers, that will be executed in the order you include them.\n\n### Feature lookup table - convert custom features to integer features\n\nThis example uses the quadratic SVM implementation [svm.js, by Andrej Karpathy](https://github.com/karpathy/svmjs). \nThis SVM (like most SVM implementations) works with integer features, so we need a way to convert our string-based features to integers.\n\n\tvar limdu = require('limdu');\n\t\n\t// First, define our base classifier type (a multi-label classifier based on svm.js):\n\tvar TextClassifier = limdu.classifiers.multilabel.BinaryRelevance.bind(0, {\n\t\tbinaryClassifierType: limdu.classifiers.SvmJs.bind(0, {C: 1.0})\n\t});\n\n\t// Initialize a classifier with a feature extractor and a lookup table:\n\tvar intentClassifier = new limdu.classifiers.EnhancedClassifier({\n\t\tclassifierType: TextClassifier,\n\t\tfeatureExtractor: limdu.features.NGramsOfWords(1),  // each word (\"1-gram\") is a feature  \n\t\tfeatureLookupTable: new limdu.features.FeatureLookupTable()\n\t});\n\t\n\t// Train and test:\n\tintentClassifier.trainBatch([\n\t\t{input: \"I want an apple\", output: \"apl\"},\n\t\t{input: \"I want a banana\", output: \"bnn\"},\n\t\t{input: \"I want chips\", output: \"cps\"},\n\t\t]);\n\t\n\tconsole.dir(intentClassifier.classify(\"I want an apple and a banana\"));  // ['apl','bnn']\n\nThe FeatureLookupTable takes care of the numbers, while you may continue to work with texts! \n\n## Serialization\n\nSay you want to train a classifier on your home computer, and use it on a remote server. To do this, you should somehow convert the trained classifier to a string, send the string to the remote server, and deserialize it there.\n\nYou can do this with the \"serialization.js\" package:\n\n\tnpm install serialization\n\t\nOn your home machine, do the following:\n\n\tvar serialize = require('serialization');\n\t\n\t// First, define a function that creates a fresh  (untrained) classifier.\n\t// This code should be stand-alone - it should include all the 'require' statements\n\t//   required for creating the classifier.\n\tfunction newClassifierFunction() {\n\t\tvar limdu = require('limdu');\n\t\tvar TextClassifier = limdu.classifiers.multilabel.BinaryRelevance.bind(0, {\n\t\t\tbinaryClassifierType: limdu.classifiers.Winnow.bind(0, {retrain_count: 10})\n\t\t});\n\t\n\t\tvar WordExtractor = function(input, features) {\n\t\t\tinput.split(\" \").forEach(function(word) {\n\t\t\t\tfeatures[word]=1;\n\t\t\t});\n\t\t};\n\t\t\n\t\t// Initialize a classifier with a feature extractor:\n\t\treturn new limdu.classifiers.EnhancedClassifier({\n\t\t\tclassifierType: TextClassifier,\n\t\t\tfeatureExtractor: WordExtractor,\n\t\t\tpastTrainingSamples: [], // to enable retraining\n\t\t});\n\t}\n\t\n\t// Use the above function for creating a new classifier:\n\tvar intentClassifier = newClassifierFunction();\n\t\n\t// Train and test:\n\tvar dataset = [\n\t\t{input: \"I want an apple\", output: \"apl\"},\n\t\t{input: \"I want a banana\", output: \"bnn\"},\n\t\t{input: \"I want chips\", output: \"cps\"},\n\t\t];\n\tintentClassifier.trainBatch(dataset);\n\t\n\tconsole.log(\"Original classifier:\");\n\tintentClassifier.classifyAndLog(\"I want an apple and a banana\");  // ['apl','bnn']\n\tintentClassifier.trainOnline(\"I want a doughnut\", \"dnt\");\n\tintentClassifier.classifyAndLog(\"I want chips and a doughnut\");  // ['cps','dnt']\n\tintentClassifier.retrain();\n\tintentClassifier.classifyAndLog(\"I want an apple and a banana\");  // ['apl','bnn']\n\tintentClassifier.classifyAndLog(\"I want chips and a doughnut\");  // ['cps','dnt']\n\t\n\t// Serialize the classifier (convert it to a string)\n\tvar intentClassifierString = serialize.toString(intentClassifier, newClassifierFunction);\n\t\n\t// Save the string to a file, and send it to a remote server.\n\n\nOn the remote server, do the following:\n\t\n\t// retrieve the string from a file and then:\n\t\n\tvar intentClassifierCopy = serialize.fromString(intentClassifierString, __dirname);\n\t\n\tconsole.log(\"Deserialized classifier:\");\n\tintentClassifierCopy.classifyAndLog(\"I want an apple and a banana\");  // ['apl','bnn']\n\tintentClassifierCopy.classifyAndLog(\"I want chips and a doughnut\");  // ['cps','dnt']\n\tintentClassifierCopy.trainOnline(\"I want an elm tree\", \"elm\");\n\tintentClassifierCopy.classifyAndLog(\"I want doughnut and elm tree\");  // ['dnt','elm']\n\nCAUTION: Serialization was not tested for all possible combinations of classifiers and enhancements. Use carefully!\n\n## Cross-validation\n\n\t// create a dataset with a lot of input-output pairs:\n\tvar dataset = [ ... ];\n\t\n\t// Decide how many folds you want in your   k-fold cross-validation:\n\tvar numOfFolds = 5;\n\n\t// Define the type of classifier that you want to test:\n\tvar IntentClassifier = limdu.classifiers.EnhancedClassifier.bind(0, {\n\t\tclassifierType: limdu.classifiers.multilabel.BinaryRelevance.bind(0, {\n\t\t\tbinaryClassifierType: limdu.classifiers.Winnow.bind(0, {retrain_count: 10})\n\t\t}),\n\t\tfeatureExtractor: limdu.features.NGramsOfWords(1),\n\t});\n\t\n\tvar microAverage = new limdu.utils.PrecisionRecall();\n\tvar macroAverage = new limdu.utils.PrecisionRecall();\n\t\n\tlimdu.utils.partitions.partitions(dataset, numOfFolds, function(trainSet, testSet) {\n\t\tconsole.log(\"Training on \"+trainSet.length+\" samples, testing on \"+testSet.length+\" samples\");\n\t\tvar classifier = new IntentClassifier();\n\t\tclassifier.trainBatch(trainSet);\n\t\tlimdu.utils.test(classifier, testSet, /* verbosity = */0,\n\t\t\tmicroAverage, macroAverage);\n\t});\n\t\n\tmacroAverage.calculateMacroAverageStats(numOfFolds);\n\tconsole.log(\"\\n\\nMACRO AVERAGE:\"); console.dir(macroAverage.fullStats());\n\t\n\tmicroAverage.calculateStats();\n\tconsole.log(\"\\n\\nMICRO AVERAGE:\"); console.dir(microAverage.fullStats());\n\n\n## Back-classification (aka Generation)\n\nUse this option to get the list of all samples with a given class.\n\n\tvar intentClassifier = new limdu.classifiers.EnhancedClassifier({\n\t\tclassifierType: limdu.classifiers.multilabel.BinaryRelevance.bind(0, {\n\t\t\tbinaryClassifierType: limdu.classifiers.Winnow.bind(0, {retrain_count: 10})\n\t\t}),\n\t\tfeatureExtractor: limdu.features.NGramsOfWords(1),\n\t\tpastTrainingSamples: [],\n\t});\n\t\n\t// Train and test:\n\tintentClassifier.trainBatch([\n\t\t{input: \"I want an apple\", output: \"apl\"},\n\t\t{input: \"I want a banana\", output: \"bnn\"},\n\t\t{input: \"I really want an apple\", output: \"apl\"},\n\t\t{input: \"I want a banana very much\", output: \"bnn\"},\n\t\t]);\n\t\n\tconsole.dir(intentClassifier.backClassify(\"apl\"));  // [ 'I want an apple', 'I really want an apple' ]\n\n\n## SVM wrappers\n\nThe native svm.js implementation takes a lot of time to train -  quadratic in the number of training samples. \nThere are two common packages that can be trained in time linear in the number of training samples. They are:\n\n* [SVM-Perf](http://www.cs.cornell.edu/people/tj/svm_light/svm_perf.html) - by Thorsten Joachims;\n* [LibLinear](http://www.csie.ntu.edu.tw/~cjlin/liblinear) - Fan, Chang, Hsieh, Wang and Lin.\n\nThe limdu.js package provides wrappers for these implementations. \nIn order to use the wrappers, you must have the binary file used for training in your path, that is:\n\n* **svm\\_perf\\_learn** - from [SVM-Perf](http://www.cs.cornell.edu/people/tj/svm_light/svm_perf.html).\n* **liblinear\\_train** - from [LibLinear](http://www.csie.ntu.edu.tw/~cjlin/liblinear).\n\nOnce you have any one of these installed, you can use the corresponding classifier instead of any binary classifier\nused in the previous demos, as long as you have a feature-lookup-table. For example, with SvmPerf:\n\n\tvar intentClassifier = new limdu.classifiers.EnhancedClassifier({\n\t\tclassifierType: limdu.classifiers.multilabel.BinaryRelevance.bind(0, {\n\t\t\tbinaryClassifierType: limdu.classifiers.SvmPerf.bind(0, \t{\n\t\t\t\tlearn_args: \"-c 20.0\" \n\t\t\t})\n\t\t}),\n\t\tfeatureExtractor: limdu.features.NGramsOfWords(1),\n\t\tfeatureLookupTable: new limdu.features.FeatureLookupTable()\n\t});\n\nand similarly with SvmLinear.\n\nSee the files classifiers/svm/SvmPerf.js and classifiers/svm/SvmLinear.js for a documentation of the options.\n\n\n## Undocumented featuers\n\nSome advanced features are not documented yet. If you need them, open an issue and I will try to document them.\n\n* Custom input normalization, based on regular expressions.\n* Input segmentation for multi-label classification - both manual (with regular expressions) and automatic.\n* Feature extraction for model adaptation.\n* Spell-checker features. \n* Hypernym features.\n* Classification based on a cross-lingual language model.\n* Format conversion - ARFF, JSON, svm-light, TSV.\n\n## Contributions\n\nAll contributions are welcome! All reasonable pull requests, with appropriate unit-tests, will be accepted.\n\n## License\n\nLGPL\n\n",
  "readmeFilename": "README.md",
  "bugs": {
    "url": "https://github.com/erelsgl/limdu/issues"
  },
  "_id": "limdu@0.3.0",
  "_from": "limdu@latest"
}
